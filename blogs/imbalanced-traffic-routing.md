# imbalanced-traffic-routing

## 2023-10-01T13:24:49.898Z

## Written by Angus Gibbs, Engineering Lead @ HubSpot.The Workflows engine at HubSpot has to handle traffic from millions of workflows every day. Huge traffic spikes and slow response times from our dependencies are a fact of life. In this post, we‚Äôll take a look at how we use Kafka swimlanes to keep Workflows fast and reliable.
_________________

Background
HubSpot‚Äôs customers use workflows to automate their business processes. Workflows are made of triggers, which tell the workflow when to ‚Äúenroll‚Äù records to be processed, and a collection of actions, which tell the workflow what to do with those enrolled records. There are millions of active workflows, which collectively execute hundreds of millions of actions every day, and tens of thousands of actions per second.

Workflows engine overview
Once the workflow gets triggered, an enrollment is created and the workflows engine begins to execute the actions in the workflow. In theory, this could happen synchronously in the same process that received the trigger. However, at the scale that HubSpot operates, that approach would quickly break down: we don‚Äôt have control over when workflows are triggered, so if the triggers came in too quickly, we‚Äôd overflow our thread pools and start dropping enrollments. Not good!
Kafka to the rescue
Instead, the enrollment and execution systems communicate using Apache Kafka, a queueing system that‚Äôs used heavily at HubSpot. Kafka allows us to decouple when a task is originally requested from when it‚Äôs actually processed; messages are produced onto a topic by a producer, and then consumed by one or more consumers. The producer and the consumer are independent of each other: the producer doesn‚Äôt know when the consumer will actually process the message, just that the consumer is guaranteed to receive the message.
Looking at our earlier diagram, we can see that each of those arrows are actually a Kafka topic:

Workflows engine with Kafka topics labeled
Now, we‚Äôve solved the problem of trying to process every trigger synchronously: we can accept triggers as fast as they occur, and we‚Äôll process them as fast as we can in our Kafka consumer. But we‚Äôve opened ourselves up to a new problem: if we don‚Äôt process messages fast enough, our Kafka consumer will build up lag, meaning that there‚Äôs a delay between when the actions are supposed to execute and when they actually execute.
If there‚Äôs a sudden burst of messages produced onto our topic, there will be a backlog of messages that we have to work through. We could scale our number of consumer instances up, but that would increase our infrastructure costs; we could add autoscale, but adding new instances takes time, and customers generally expect workflows to process enrollments in near real time. So what do we do?
¬†
Enter, swimlanes
The fundamental problem here is that all of our traffic, for all of our customers, is being produced to the same queue. If the consumer of that queue experiences delays, then all of our traffic is delayed. Introducing a swimlane allows us to isolate slices of that traffic:

Swimlanes for the workflow execution system
Instead of sending all traffic to our ‚Äúreal time‚Äù traffic swimlane‚Äîwhere we want to minimize delays as much as possible‚Äîwe can send some of our traffic to an ‚Äúoverflow‚Äù swimlane. The two swimlanes process messages in exactly the same way, but can build up delays independently. If we get a sudden burst of traffic that‚Äôs coming in faster than the real time swimlane can accommodate, we‚Äôll protect the real time swimlane by sending excess traffic to the overflow swimlane.
At face value this might seem like we‚Äôre just moving the problem somewhere else‚Äîand in some ways we are‚Äîbut this strategy has been widely deployed at HubSpot with lots of success. Usually only a small handful of customers are generating a burst of traffic at any point in time. By moving the traffic elsewhere we‚Äôre isolating those customers from the thousands of other customers with more stable traffic levels, providing a much faster experience for the majority of our customers.
There are many strategies that can be used to determine which swimlane to route a message to. In general, these can be classified into ‚Äúmanual‚Äù (reactive) and ‚Äúautomatic‚Äù (proactive) strategies. Automatic strategies reduce operational burden, since they don‚Äôt require any intervention from an engineer, but manual strategies are helpful too, since they give us an ‚Äúescape hatch‚Äù if we ever need to reroute a subset of our traffic.
Automatically handling bursts
We know that we want to route bursts of traffic to the overflow swimlane, but how do we actually detect the bursts? It‚Äôs not always possible, but sometimes we can tell just by looking at the fields on the Kafka messages. For example, workflows has a feature called ‚Äúbulk enrollment‚Äù that allows customers to quickly enroll millions of records into a workflow. We can look at the original enrollment source on the Kafka message, and if it‚Äôs a bulk enrollment, automatically route to the overflow swimlane. Another common pattern at HubSpot is to have a ‚Äúbackfill‚Äù flag on the Kafka message, to indicate that a message is being produced as a result of a one-off job that doesn‚Äôt need to be processed right away; this can also be used for swimlane routing.
Sometimes, though, we can‚Äôt tell just by looking at the message. In those cases, a rate limiter can be used. Rate limiters, such as a Guava RateLimiter, enforce a maximum rate at which some processing can happen. When we‚Äôre deciding which swimlane to route traffic to, we‚Äôll check each message against a per-customer rate limit, and route traffic to the overflow swimlane once the rate limiter starts rejecting traffic.
Rate limits are made up of a threshold value (e.g. 250 requests) and bucket size (e.g. per second or per minute). Smaller bucket sizes respond more quickly to bursts of traffic, but can over-penalize small bursts. For example, if we have a rate limit of 250 requests/sec, and we receive 300 requests in one second and then no further requests, we‚Äôll rate limit 50 requests even though the total amount of requests is pretty reasonable. To get around this, we can enforce multiple rate limits, with different thresholds and bucket sizes (e.g. one rate limit of 500 requests/sec and another of 1000 requests/min).
Setting the rate limit thresholds requires some domain knowledge. Generally we‚Äôll look at our workers‚Äô metrics when they‚Äôre under load, to get an estimate of their maximum throughput. Then, we‚Äôll set per-customer rate limits below that threshold, to protect against individual customers dominating the overall capacity. It‚Äôs a good idea to make the limits configurable, so that they can be changed over time based on observed throughput and lag.
Other ways to automatically route traffic
So far we‚Äôve only talked about swimlanes as a way to offload bursts of traffic, but they‚Äôre actually much more versatile than that. Since workflow execution traffic is very heterogeneous, we also leverage swimlanes to isolate ‚Äúfast‚Äù traffic from ‚Äúslow‚Äù traffic. Similar to how we look at the original enrollment source to decide which swimlane to route to, we‚Äôll also look at what type of action we‚Äôre executing, and we‚Äôll execute action types that generally take longer (e.g. custom code actions, which allow customers to write arbitrary Node or Python scripts) in dedicated swimlanes. We even have a system that can predict whether an action will be slow based on its historical latency, which might get its own blog post one day üòâ.
Manual traffic routing
All of the automatic routing means that most of the time, the workflows engine runs on autopilot. But every now and then an issue comes up that requires some manual intervention, and when that happens, we‚Äôre happy to have manual routing options. One manual strategy that we make use of frequently is a configurable list of customers to forcibly re-route. If something goes wrong for a specific customer‚Äîfor example, actions executing particularly slowly‚Äîwe can isolate that customer into its own swimlane, to reduce the impact of the issue while we investigate and resolve the problem.
In order for manual routing to be effective, it‚Äôs important to have good visibility into our systems‚Äîthat way we know what subset of traffic needs to get rerouted. Luckily for us, HubSpot has world class internal developer tooling, which makes it easy to uncover problems by searching our logs or looking at our metrics. We record custom metrics that give us execution latency across many dimensions (e.g. action type), and we have logging that tells us what‚Äôs happening for every action.
Conclusion
Kafka is a powerful tool for asynchronous task processing, but because queues are shared infrastructure, there‚Äôs no isolation between messages that are produced to the queue. Swimlanes give us a way to isolate traffic. We‚Äôve laid out the basic principles behind our swimlanes, but these patterns can be applied in many different ways. Altogether we have about a dozen different swimlanes powering our execution engine, helping to keep workflows fast and reliable.
Are you an engineer who's interested in solving hard problems like these? Check out our careers page for your next opportunity! And to learn more about our culture, follow us on Instagram @HubSpotLife.
